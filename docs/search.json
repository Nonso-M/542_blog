[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Altered Image of Scrapy and AOT Characters\n\n\nIf you’re new to web scraping and want a powerful tool to automate your data collection tasks, look no further than Scrapy. This Python-based framework makes it easy to crawl websites, extract data, and store it in your desired format. In this blog post, we’ll walk you through scraping a website with Scrapy step by step. This part focuses on setting up a Scrapy project locally and running it in the terminal. Most scrapy projects are ran from the terminal using the command scrapy crawlIf you are new to Scrapy and also an Attack on Titan fan, well, today is your day!\n\n\nTL;DR Scrapy is an open-source Python framework used for web scraping and crawling. It provides a set of tools and libraries that simplify the process of extracting data from websites. With Scrapy, developers can define the structure of a website, specify how to navigate its pages, and extract desired data by writing Python code.\nIt supports various features such as handling cookies, managing sessions, following links, and handling form submissions. Scrapy also includes powerful mechanisms for handling asynchronous requests and managing concurrency, allowing for efficient and fast scraping of multiple pages simultaneously.\nScrapy is revered for being an asynchronous library built on top of Twisted and also being a full engine where you can scrape, parse the data, perform further preprocessing using itemloaders, and store processed data within a database."
  },
  {
    "objectID": "index.html#what-is-scrapy",
    "href": "index.html#what-is-scrapy",
    "title": "Introduction",
    "section": "",
    "text": "TL;DR Scrapy is an open-source Python framework used for web scraping and crawling. It provides a set of tools and libraries that simplify the process of extracting data from websites. With Scrapy, developers can define the structure of a website, specify how to navigate its pages, and extract desired data by writing Python code.\nIt supports various features such as handling cookies, managing sessions, following links, and handling form submissions. Scrapy also includes powerful mechanisms for handling asynchronous requests and managing concurrency, allowing for efficient and fast scraping of multiple pages simultaneously.\nScrapy is revered for being an asynchronous library built on top of Twisted and also being a full engine where you can scrape, parse the data, perform further preprocessing using itemloaders, and store processed data within a database."
  },
  {
    "objectID": "index.html#step-1-setting-up-your-enviroment-and-instally-scrapy",
    "href": "index.html#step-1-setting-up-your-enviroment-and-instally-scrapy",
    "title": "Introduction",
    "section": "Step 1: Setting up your Enviroment and Instally Scrapy:",
    "text": "Step 1: Setting up your Enviroment and Instally Scrapy:\nIt is recommended practice to construct a separate virtual environment for each of your Python projects in order to prevent version conflicts in the future. This ensures that any packages you install for one project are kept apart from those for other projects, preventing accidental project breakage. These commands may be slightly different on your machine, depending on the operating system.\n\n\n\nDoggo scared of peole that dont use Virtual Environments (Image from Pintrest)\n\n\nThe following method is derived from scrapeops and the official scrapy documentation.\n\nMacOS/Linux\nSetting up a virtual environment on MacOS or any Linux distro uses the following commands. \n$ sudo apt install -y python3-venv\nNext, we create a virtual environment in our project directory.\n$ cd /&lt;folder name&gt;\n$ python3 -m venv venv\n$ source venv/bin/activate\nIn our virtual environment, we will then install Scrapy using the following command:\n$ apt-get install python3-pip\n$ sudo pip3 install scrapy\nNavigate to the project folder where you want to create the virtual environment, create the virtual environment, and activate it. The following commands achieve this\ncd /&lt;folder_name&gt;\npython&lt;version&gt; -m venv &lt;virtual-environment-name&gt;\nFinally we activate the virtual environment and install scrapy\nsource &lt;name of virtual env&gt;\\Scripts\\activate\npip install scrapy\nif you are using anaconda use the cell below\nconda activate &lt;enviroment_name&gt;\nconda install -c conda-forge scrapy"
  },
  {
    "objectID": "index.html#step-2-setting-up-our-aot-scrapy-project",
    "href": "index.html#step-2-setting-up-our-aot-scrapy-project",
    "title": "Introduction",
    "section": "Step 2 — Setting up our AOT Scrapy Project",
    "text": "Step 2 — Setting up our AOT Scrapy Project\nWe may move on to the enjoyable aspects now that our setting is set up. construction of the first Scrapy spider! Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items).\nThis project will consist of three spiders\n\naot_locations: this spider would be in charge or scraping the location page on AOT fan weblink site\naot_organizations: This spider would be in charge of scraping AOT organization site link\naot_titans: This spider would scrape informations about the different titans on the show(Link)\n\nOpen your command line and type in the following CL command:\n#we can call the project aot in our case\nscrapy startproject &lt;project_name&gt;\n\nThe Scrapy Project Structure​\nScrapy has a generic project structure when you create a new project. After running the scrapy startproject command, it generates a file structure The folder structure looks something like this\n├── scrapy.cfg\n└── &lt;project_name&gt;\n    ├── __init__.py\n    ├── items.py\n    ├── middlewares.py\n    ├── pipelines.py\n    ├── settings.py\n    └── spiders\n        └── __init__.py\nAll of your project settings, such as enabling pipelines, middlewares, and other components, are located in settings.py. The latency, concurrency, and a lot more things can all be changed here.\nThe item.py file is used to define the preprocessing pipeline model for the extracted data. You can create a special model that inherits from the Scrapy Item class. The scrapy.cfg is a configuration file to change some deployment settings, etc.\n\n\nWriting the Spider\nI will quickly go over the structure and logic of one of the spiders on this Attack on Titans scrapy project to get a grasp of it. Navigate to the spiders directory and create a new spider file:\ncd quotes_scraper/quotes_scraper/spiders\ntouch aot_spider.py\nEdit quotes_spider.py to define your spider:\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom ..items import AotTitanItem\n\n\nclass AotTitansSpider(scrapy.Spider):\n    \"\"\"\n    A spider for scraping data about titans from the Attack on Titan Fandom wiki\n\n    Attributes:\n        name: name of the spider, string for identifying this spider\n        allowed_domains: list containing the domains allowed for the spiders\n        start_urls: list of the urls to begin scraping\n        custom_settings: dictionary of settings for this specific spider\n    \"\"\"\n\n    name = \"aot_titans\"\n    allowed_domains = [\"attackontitan.fandom.com\"]\n    start_urls = [\n        \"https://attackontitan.fandom.com/wiki/Colossal_Titan_(Anime)\",\n        \"https://attackontitan.fandom.com/wiki/Armored_Titan_(Anime)\",\n    ]\n    custom_settings = {\n        \"FEEDS\": {\"./aot_scraper/scrapes/titan_data.jl\": {\"format\": \"jsonlines\"}},\n    }\n\n    def parse(self, response):\n        \"\"\"\n        Function for scraping titan information from the start_urls,\n        and supplying scraped data to the item loader\n\n        Args:\n            response: response from start_urls\n        \"\"\"\n\n        # page section with information to scrape\n        info_block = response.css(\n            \"div#mw-content-text aside.portable-infobox.pi-background.pi-border-color.pi-theme-wikia.pi-layout-default\"\n        )\n\n        # specific HTML elements with content\n        height_div = info_block.xpath(\".//div[@data-source='Height']\")\n        powers_div = info_block.xpath(\".//div[@data-source='Abilities']\")\n        current_shifter = info_block.xpath(\n            \".//div[@data-source='Current inheritor(s)']\"\n        )\n        former_shifter = info_block.xpath(\".//div[@data-source='Former inheritor(s)']\")\n\n        # text retrieval from HTML\n        height_data = height_div.css(\"div.pi-data-value.pi-font *::text\").get()\n        powers_data = powers_div.css(\"div.pi-data-value.pi-font *::text\").getall()\n        current_shifters_data = current_shifter.css(\n            \"div.pi-data-value.pi-font a::text\"\n        ).getall()\n        former_shifters_data = former_shifter.css(\n            \"div.pi-data-value.pi-font a::text\"\n        ).getall()\n\n        # instantiate item loader\n        titan_item_loader = ItemLoader(item=AotTitanItem(), selector=info_block)\n\n        # use item loader to populate fields\n        titan_item_loader.add_css(\"name\", \"div.pi-data-value.pi-font::text\")\n        titan_item_loader.add_value(\"height\", height_data)\n        titan_item_loader.add_value(\"powers\", powers_data)\n        titan_item_loader.add_value(\n            \"shifters\", current_shifters_data + former_shifters_data\n        )\n\n        yield titan_item_loader.load_item()\nKey Components of the Spider\nSPIDER ATRRIBUTES\n\nname: The name of the spider (used to run the spider). In this case, it’s “aot_titans”.\nallowed_domains: Specifies the domains the spider is allowed to crawl. This prevents the spider from inadvertently accessing unrelated websites.Here, it restricts scraping to attackontitan.fandom.com.\nstart_urls: Contains the list of URLs the spider will visit initially. Each URL corresponds to a specific Titan’s page in the wiki.\ncustom_settings: Overrides default Scrapy settings for this spider. In this case, The scraped data is saved in JSON Lines format (.jl) to the file path ./aot_scraper/scrapes/titan_data.jl.\n\nPARSE METHOD\n\nThe parse method is the heart of the spider. It processes the response received from each URL in start_urls and extracts relevant data.\ninfo_block = response.css(\n    \"div#mw-content-text aside.portable-infobox.pi-background.pi-border-color.pi-theme-wikia.pi-layout-default\"\n)\nUses a CSS selector to locate the HTML section (aside.portable-infobox) containing Titan-related details.\nThis block acts as the primary source of structured data on the page.\n\nSCRAPY ITEM\ntitan_item_loader = ItemLoader(item=AotTitanItem(), selector=info_block)\n\nThe ItemLoader simplifies the process of populating the Scrapy AotTitanItem object.\nThe fields in the item are populated with extracted data:\ntitan_item_loader.add_css(\"name\", \"div.pi-data-value.pi-font::text\")\ntitan_item_loader.add_value(\"height\", height_data)\ntitan_item_loader.add_value(\"powers\", powers_data)\ntitan_item_loader.add_value(\"shifters\", current_shifters_data + former_shifters_data)\n\nRUNNING THE SPIDER This is the time to test your spider and see the outputted result. It is always adviced to look at the output result on the terminal to ensure your spider is not encountering any error. To run yoiur spider, you run:\nscrapy crawl quotes\nIf you want to output the result to a file, e.g. a csv or json file format you run a command similar to this (You can always check the scrapy documentation for the updated file format support):\n#Outputs to a json file called \"aot.py\"\nscrapy crawl quotes -o aot.json\nYou can check out the output file to see the result of your hardwork!"
  },
  {
    "objectID": "index.html#final-thoughts",
    "href": "index.html#final-thoughts",
    "title": "Introduction",
    "section": "FINAL THOUGHTS",
    "text": "FINAL THOUGHTS\nCongratulations! You’ve successfully scraped a website using Scrapy. This tutorial covered the basics, but Scrapy offers much more, including middleware, custom settings, and advanced spider types like CrawlSpider. For more advanced use cases, refer to the Scrapy documentation."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "How to Scrape a Website with Scrapy: A Beginner’s Guide",
    "section": "",
    "text": "Altered Image of Scrapy and AOT Characters"
  },
  {
    "objectID": "posts/post-with-code/index.html#what-is-scrapy",
    "href": "posts/post-with-code/index.html#what-is-scrapy",
    "title": "How to Scrape a Website with Scrapy: A Beginner’s Guide",
    "section": "What is Scrapy",
    "text": "What is Scrapy\nTL;DR Scrapy is an open-source Python framework used for web scraping and crawling. It provides a set of tools and libraries that simplify the process of extracting data from websites. With Scrapy, developers can define the structure of a website, specify how to navigate its pages, and extract desired data by writing Python code.\nIt supports various features such as handling cookies, managing sessions, following links, and handling form submissions. Scrapy also includes powerful mechanisms for handling asynchronous requests and managing concurrency, allowing for efficient and fast scraping of multiple pages simultaneously.\nScrapy is revered for being an asynchronous library built on top of Twisted and also being a full engine where you can scrape, parse the data, perform further preprocessing using itemloaders, and store processed data within a database."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-1-setting-up-your-enviroment-and-instally-scrapy",
    "href": "posts/post-with-code/index.html#step-1-setting-up-your-enviroment-and-instally-scrapy",
    "title": "How to Scrape a Website with Scrapy: A Beginner’s Guide",
    "section": "Step 1: Setting up your Enviroment and Instally Scrapy:",
    "text": "Step 1: Setting up your Enviroment and Instally Scrapy:\nIt is recommended practice to construct a separate virtual environment for each of your Python projects in order to prevent version conflicts in the future. This ensures that any packages you install for one project are kept apart from those for other projects, preventing accidental project breakage. These commands may be slightly different on your machine, depending on the operating system.\n\n\n\nDoggo scared of peole that dont use Virtual Environments (Image from Pintrest)\n\n\nThe following method is derived from scrapeops and the official scrapy documentation.\n\nMacOS/Linux\nSetting up a virtual environment on MacOS or any Linux distro uses the following commands. \n$ sudo apt install -y python3-venv\nNext, we create a virtual environment in our project directory.\n$ cd /&lt;folder name&gt;\n$ python3 -m venv venv\n$ source venv/bin/activate\nIn our virtual environment, we will then install Scrapy using the following command:\n$ apt-get install python3-pip\n$ sudo pip3 install scrapy\nNavigate to the project folder where you want to create the virtual environment, create the virtual environment, and activate it. The following commands achieve this\ncd /&lt;folder_name&gt;\npython&lt;version&gt; -m venv &lt;virtual-environment-name&gt;\nFinally we activate the virtual environment and install scrapy\nsource &lt;name of virtual env&gt;\\Scripts\\activate\npip install scrapy\nif you are using anaconda use the cell below\nconda activate &lt;enviroment_name&gt;\nconda install -c conda-forge scrapy"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-2-setting-up-our-aot-scrapy-project",
    "href": "posts/post-with-code/index.html#step-2-setting-up-our-aot-scrapy-project",
    "title": "How to Scrape a Website with Scrapy: A Beginner’s Guide",
    "section": "Step 2 — Setting up our AOT Scrapy Project",
    "text": "Step 2 — Setting up our AOT Scrapy Project\nWe may move on to the enjoyable aspects now that our setting is set up. construction of the first Scrapy spider! Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items).\nThis project will consist of three spiders\n\naot_locations: this spider would be in charge or scraping the location page on AOT fan weblink site\naot_organizations: This spider would be in charge of scraping AOT organization site link\naot_titans: This spider would scrape informations about the different titans on the show(Link)\n\nOpen your command line and type in the following CL command:\n#we can call the project aot in our case\nscrapy startproject &lt;project_name&gt;\n\nThe Scrapy Project Structure​\nScrapy has a generic project structure when you create a new project. After running the scrapy startproject command, it generates a file structure The folder structure looks something like this\n├── scrapy.cfg\n└── &lt;project_name&gt;\n    ├── __init__.py\n    ├── items.py\n    ├── middlewares.py\n    ├── pipelines.py\n    ├── settings.py\n    └── spiders\n        └── __init__.py\nAll of your project settings, such as enabling pipelines, middlewares, and other components, are located in settings.py. The latency, concurrency, and a lot more things can all be changed here.\nThe item.py file is used to define the preprocessing pipeline model for the extracted data. You can create a special model that inherits from the Scrapy Item class. The scrapy.cfg is a configuration file to change some deployment settings, etc.\n\n\nWriting the Spider\nI will quickly go over the structure and logic of one of the spiders on this Attack on Titans scrapy project to get a grasp of it. Navigate to the spiders directory and create a new spider file:\ncd quotes_scraper/quotes_scraper/spiders\ntouch aot_spider.py\nEdit quotes_spider.py to define your spider:\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom ..items import AotTitanItem\n\n\nclass AotTitansSpider(scrapy.Spider):\n    \"\"\"\n    A spider for scraping data about titans from the Attack on Titan Fandom wiki\n\n    Attributes:\n        name: name of the spider, string for identifying this spider\n        allowed_domains: list containing the domains allowed for the spiders\n        start_urls: list of the urls to begin scraping\n        custom_settings: dictionary of settings for this specific spider\n    \"\"\"\n\n    name = \"aot_titans\"\n    allowed_domains = [\"attackontitan.fandom.com\"]\n    start_urls = [\n        \"https://attackontitan.fandom.com/wiki/Colossal_Titan_(Anime)\",\n        \"https://attackontitan.fandom.com/wiki/Armored_Titan_(Anime)\",\n    ]\n    custom_settings = {\n        \"FEEDS\": {\"./aot_scraper/scrapes/titan_data.jl\": {\"format\": \"jsonlines\"}},\n    }\n\n    def parse(self, response):\n        \"\"\"\n        Function for scraping titan information from the start_urls,\n        and supplying scraped data to the item loader\n\n        Args:\n            response: response from start_urls\n        \"\"\"\n\n        # page section with information to scrape\n        info_block = response.css(\n            \"div#mw-content-text aside.portable-infobox.pi-background.pi-border-color.pi-theme-wikia.pi-layout-default\"\n        )\n\n        # specific HTML elements with content\n        height_div = info_block.xpath(\".//div[@data-source='Height']\")\n        powers_div = info_block.xpath(\".//div[@data-source='Abilities']\")\n        current_shifter = info_block.xpath(\n            \".//div[@data-source='Current inheritor(s)']\"\n        )\n        former_shifter = info_block.xpath(\".//div[@data-source='Former inheritor(s)']\")\n\n        # text retrieval from HTML\n        height_data = height_div.css(\"div.pi-data-value.pi-font *::text\").get()\n        powers_data = powers_div.css(\"div.pi-data-value.pi-font *::text\").getall()\n        current_shifters_data = current_shifter.css(\n            \"div.pi-data-value.pi-font a::text\"\n        ).getall()\n        former_shifters_data = former_shifter.css(\n            \"div.pi-data-value.pi-font a::text\"\n        ).getall()\n\n        # instantiate item loader\n        titan_item_loader = ItemLoader(item=AotTitanItem(), selector=info_block)\n\n        # use item loader to populate fields\n        titan_item_loader.add_css(\"name\", \"div.pi-data-value.pi-font::text\")\n        titan_item_loader.add_value(\"height\", height_data)\n        titan_item_loader.add_value(\"powers\", powers_data)\n        titan_item_loader.add_value(\n            \"shifters\", current_shifters_data + former_shifters_data\n        )\n\n        yield titan_item_loader.load_item()\nKey Components of the Spider\nSPIDER ATRRIBUTES\n\nname: The name of the spider (used to run the spider). In this case, it’s “aot_titans”.\nallowed_domains: Specifies the domains the spider is allowed to crawl. This prevents the spider from inadvertently accessing unrelated websites.Here, it restricts scraping to attackontitan.fandom.com.\nstart_urls: Contains the list of URLs the spider will visit initially. Each URL corresponds to a specific Titan’s page in the wiki.\ncustom_settings: Overrides default Scrapy settings for this spider. In this case, The scraped data is saved in JSON Lines format (.jl) to the file path ./aot_scraper/scrapes/titan_data.jl.\n\nPARSE METHOD\n\nThe parse method is the heart of the spider. It processes the response received from each URL in start_urls and extracts relevant data.\ninfo_block = response.css(\n    \"div#mw-content-text aside.portable-infobox.pi-background.pi-border-color.pi-theme-wikia.pi-layout-default\"\n)\nUses a CSS selector to locate the HTML section (aside.portable-infobox) containing Titan-related details.\nThis block acts as the primary source of structured data on the page.\n\nSCRAPY ITEM\ntitan_item_loader = ItemLoader(item=AotTitanItem(), selector=info_block)\n\nThe ItemLoader simplifies the process of populating the Scrapy AotTitanItem object.\nThe fields in the item are populated with extracted data:\ntitan_item_loader.add_css(\"name\", \"div.pi-data-value.pi-font::text\")\ntitan_item_loader.add_value(\"height\", height_data)\ntitan_item_loader.add_value(\"powers\", powers_data)\ntitan_item_loader.add_value(\"shifters\", current_shifters_data + former_shifters_data)\n\nRUNNING THE SPIDER This is the time to test your spider and see the outputted result. It is always adviced to look at the output result on the terminal to ensure your spider is not encountering any error. To run yoiur spider, you run:\nscrapy crawl quotes\nIf you want to output the result to a file, e.g. a csv or json file format you run a command similar to this (You can always check the scrapy documentation for the updated file format support):\n#Outputs to a json file called \"aot.py\"\nscrapy crawl quotes -o aot.json\nYou can check out the output file to see the result of your hardwork!"
  },
  {
    "objectID": "posts/post-with-code/index.html#final-thoughts",
    "href": "posts/post-with-code/index.html#final-thoughts",
    "title": "How to Scrape a Website with Scrapy: A Beginner’s Guide",
    "section": "FINAL THOUGHTS",
    "text": "FINAL THOUGHTS\nCongratulations! You’ve successfully scraped a website using Scrapy. This tutorial covered the basics, but Scrapy offers much more, including middleware, custom settings, and advanced spider types like CrawlSpider. For more advanced use cases, refer to the Scrapy documentation."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]